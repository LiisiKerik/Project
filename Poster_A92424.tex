%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\documentclass{tikzposter}
\usepackage{amsfonts}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{cormorantgaramond}
\usepackage[T1]{fontenc}
\usepackage{graphicx}
\usepackage[utf8]{inputenc}
\usepackage{pifont}
\usepackage{tcolorbox}
\usepackage{xcolor}
\DeclareMathOperator{\adj}{adj}
\DeclareMathOperator{\toepl}{Toepl}
\def\kui{\quad\text{if}\quad}
\definecolor{antiquewhite}{RGB}{250,235,215}
\definecolor{coral}{RGB}{255,127,80}
\definecolor{crimson}{RGB}{220,20,60}
\definecolor{lightcoral}{RGB}{255,127,80}
\definecolorstyle{myColorStyle}{}{
  \colorlet{backgroundcolor}{antiquewhite}
  \colorlet{blocktitlebgcolor}{coral}
  \colorlet{framecolor}{coral}}
\newcommand\pilt[1]{
  \begin{center}
    \begin{tcolorbox}[arc=4mm,boxrule=2mm,colback=white,colframe=lightcoral,width=0.38\textwidth]
      \includegraphics[width=\textwidth]{#1}
    \end{tcolorbox}
  \end{center}}
\renewcommand\labelitemi{\color{crimson}{\ding{96}}}
\usecolorstyle{myColorStyle}
\title{Berkowitz Algorithm}
\institute{University of Tartu, Institute of Computer Science}
\author{Liisi Kerik}
\begin{document}
  \maketitle
  \begin{columns}
    \column{0.5}
      \block{The Algorithm}{
        Berkowitz algorithm finds the characteristic polynomial of a matrix. An $n\times n$ square matrix $A$, where $n>0$, can be written as
        \begin{equation*}
          A=\left(\begin{tabular}{c|c}$a$&$\tau$\\\hline$\lambda$&$A'$\end{tabular}\right)
        \end{equation*}
        where $a$ is the top left element, $\tau$ is the rest of the top row, $\lambda$ is the rest of the left column, and $A'$ is the rest of the matrix. The characteristic polynomial of $A$ can be expressed as
        \begin{equation*}
          c=\begin{cases}(1)\kui n=0\\\toepl(-1,a,\tau\lambda,...,\tau A'^{n-2}\lambda)\cdot(0|c')\kui n>0\end{cases}
        \end{equation*}
        where $c'$ is the characteristic polynomial of $A'$ and $\toepl$ denotes an $(n+1)\times(n+1)$ Toeplitz triangular matrix:
        \begin{equation*}
          \toepl(x_0,...,x_n)_{ij}=\begin{cases}x_{j-i}\kui j-i\geqslant0\\0\kui j-i<0\end{cases}
        \end{equation*}
        Adjugate and determinant can be found from the characteristic polynomial using the following equations:
        \begin{equation*}
          \adj A=-\sum_{i=1}^nc_iA^{i-1}
        \end{equation*}
        \begin{equation*}
          \det A=c_0
        \end{equation*}
        From this, we can find the inverse
        \begin{equation*}
          A^{-1}=\dfrac{\adj A}{\det A}
        \end{equation*}
        If we need only the determinant, we can find it more efficiently, skipping the computation of the last characteristic polynomial:
        \begin{equation*}
          \det A=\begin{cases}1\kui n=0\\a\cdot\det A'-\tau\cdot\adj A'\cdot\lambda\kui n>0\end{cases}
        \end{equation*}
        \begin{thebibliography}{9}
          \bibitem{rot}
            G. Rote. \textit{Division-Free Algorithms for the Determinant and the Pfaffian: Algebraic and Combinatorial Approaches}
          \bibitem{sol}
            M. Soltys. \textit{Berkowitz's Algorithm and Clow Sequences}
        \end{thebibliography}}
      \block{Precision}{
        We compared Berkowitz algorithm to Gaussian elimination. For each algorithm and matrix size, we performed $10^4$ precision measurements on uniformly random matrices of double-precision floating point numbers in $[-1,1]$ and took the median error. Precision was measured by comparing the approximate results (floating point) to the exact answers (fractions).

        \pilt{Error_Det}

        For determinant, we used the relative error with respect to the exact result.

        \pilt{Error_Inv}

        For inverse, we used the Euclidean distance from the exact result.}
    \column{0.5}
      \block{Performance}{
        Berkowitz algorithm and Gaussian elimination were implemented in Python, without the aid of any special libraries like NumPy, and without making use of concurrency. For each algorithm and matrix size, we measured how many uniformly random matrices of double-precision floating point numbers in $[-1,1]$ can be processed in one second and took the average running time.

        \pilt{Time_Det}

        \pilt{Time_Inv}}
      \block{Disadvantages}{
        \begin{itemize}
          \item
            Precision is worse than that of Gaussian elimination. If the algorithm has to be implemented on top of some limited-precision number format like floating point numbers (which is likely the case for most standard uses), then the loss of precision is a disadvantage, especially for larger matrices. In some less standard uses, for example if the ring is not $\mathbb{R}$ but $\mathbb{Z}_n$, precision is not an issue.
          \item
            Unparallelised performance is significantly worse than that of Gaussian elimination. (Implementing a parallelised version was outside the scope of this work.)
        \end{itemize}}
      \block{Interesting Features, Advantages and Uses}{
        \begin{itemize}
          \item
            The algorithm, unlike Gaussian elimination, does not require modifying the matrix, and is therefore side effect free and well-suited for implementing in a purely functional language like Haskell or Idris, without resorting to essentially imperative code wrapped inside a monad to contain side effects.
          \item
            The algorithm does not require random access to the elements of the matrix. Rows and columns are processed from head to tail. This makes it straightforward to implement the algorithm on top of a data structure without random access.
          \item
            The algorithm is division-free. This is an advantage when we are working with a ring where not all elements have inverses, or when division is slow or imprecise.
          \item
            For any given size of the matrix, the algorithm can be written as a directed acyclic graph (DAG). The algorithm does not branch depending on the contents of the matrix. The simplicity of the algorithm makes bugs less likely. Absence of branching might make the algorithm suitable, without fundamental modifications, for applications where branching is undesirable or even dangerous because it might leak information (secure multiparty computation and other side channel safe code).
          \item
            The algorithm is an excellent teaching tool for learning linear algebra. Studying the algorithm confers a better understanding of several fundamental linear algebra concepts - such as characteristic polynomial, adjugate and determinant - and their relationships.
          \item
            The algorithm parallelises well - the circuit is of polynomial size and $\mathcal{O}(\log^2)$ depth~\cite{sol}.
        \end{itemize}}
  \end{columns}
\end{document}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%